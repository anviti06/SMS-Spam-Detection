{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anviti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.tree import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel():\n",
    "    #1.Text Processing\n",
    "    df = pd.read_table('SMSSpamCollection.txt', header=None)\n",
    "    y = df[0]\n",
    "    #print(df.head())\n",
    "    #df.info()\n",
    "    #Label encoders -converting Spam as 1 and Ham as 0\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    #replacing SMS Data into original place\n",
    "    raw_text = df[1]\n",
    "    #2. Text Processing\n",
    "\n",
    "    processed = raw_text.str.replace(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "                                     'emailaddr')\n",
    "    processed = processed.str.replace(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "                                      'httpaddr')\n",
    "    processed = processed.str.replace(r'£|\\$', 'moneysymb')    \n",
    "    processed = processed.str.replace(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr')    \n",
    "    processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
    "    processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "    processed = processed.str.replace(r'\\s+', ' ')\n",
    "    processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n",
    "    processed = processed.str.lower()\n",
    "\n",
    "    #removing stop words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    processed = processed.apply(lambda x: ' '.join(\n",
    "        term for term in x.split() if term not in set(stop_words))\n",
    "    )\n",
    "\n",
    "    #stemming\n",
    "    porter = nltk.PorterStemmer()\n",
    "    processed = processed.apply(lambda x: ' '.join(\n",
    "        porter.stem(term) for term in x.split())\n",
    "    )\n",
    "\n",
    "    #3. Feature ENgineering\n",
    "    #3.1 Tokenization - using tf-idf static to create a matrix with each row representing training example and each col \n",
    "    #    representing the tf-idf value of nth gram word(here we use 1 gram and 2 gram , i.e taking 1 work at a time , \n",
    "    #    and taking two words at a time). This is also called vectorization\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X_ngrams = vectorizer.fit_transform(processed)\n",
    "    #X_ngrams.shape #pretty big matrix with 36348 cols showing 1gram or 2-gram words\n",
    "    #4. Training and evaluating model\n",
    "    #SVM: finds a hyperplane that deferentiates these two classes\n",
    "\n",
    "    #Splitting into and Testing Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_ngrams,\n",
    "        y_enc,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=y_enc\n",
    "    )\n",
    "    \n",
    "    classifiers =[LinearSVC(loss='hinge'), RandomForestClassifier(),LogisticRegression(penalty='l2'),MultinomialNB() ]\n",
    "\n",
    "    #1. SVM Classifier\n",
    "    mx = 0\n",
    "    for i in classifiers:\n",
    "        i.fit(X_train, y_train)\n",
    "        y_pred = i.predict(X_test)\n",
    "        score = metrics.f1_score(y_test, y_pred)\n",
    "        if(score > mx):\n",
    "            mx = score\n",
    "            clf = i\n",
    "        print (\"Data has been trained with \" ,i , \" having score : \" ,end='')\n",
    "        print('')\n",
    "        \n",
    "    \n",
    "    return clf, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been trained with  LinearSVC(loss='hinge')  having score : \n",
      "Data has been trained with  RandomForestClassifier()  having score : \n",
      "Data has been trained with  LogisticRegression()  having score : \n",
      "Data has been trained with  MultinomialNB()  having score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearSVC(loss='hinge'), TfidfVectorizer(ngram_range=(1, 2)))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(messy_string):\n",
    "    assert(type(messy_string) == str)\n",
    "    cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', messy_string)\n",
    "    cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     cleaned)\n",
    "    cleaned = re.sub(r'£|\\$', 'moneysymb', cleaned)\n",
    "    cleaned = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr', cleaned)\n",
    "    cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "    cleaned = re.sub(r'[^\\w\\d\\s]', ' ', cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = re.sub(r'^\\s+|\\s+?$', '', cleaned.lower())\n",
    "    return ' '.join(\n",
    "        porter.stem(term) \n",
    "        for term in cleaned.split()\n",
    "        if term not in set(stop_words)\n",
    "    )\n",
    "\n",
    "def spam_filter(message):\n",
    "    clf, vectorizer = trainmodel()\n",
    "    if clf.predict(vectorizer.transform([preprocess_text(message)])):\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'not spam'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been trained with  LinearSVC(loss='hinge')  having score : \n",
      "Data has been trained with  RandomForestClassifier()  having score : \n",
      "Data has been trained with  LogisticRegression()  having score : \n",
      "Data has been trained with  MultinomialNB()  having score : \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-cea69ea9dbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspam_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ohhh, but those are the best kind of foods'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-10659bf03385>\u001b[0m in \u001b[0;36mspam_filter\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspam_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'spam'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-10659bf03385>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(messy_string)\u001b[0m\n\u001b[1;32m     14\u001b[0m     return ' '.join(\n\u001b[1;32m     15\u001b[0m         \u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-39-10659bf03385>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "spam_filter('Ohhh, but those are the best kind of foods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
